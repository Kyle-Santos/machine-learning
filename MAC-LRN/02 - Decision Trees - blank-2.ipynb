{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<span style=\"color:#448844\">Note</span>** This notebook is meant to be interactive. Launch this notebook in Jupyter to see its full potential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Exercise\n",
    "This exercise will guide you in implementing the Decision Trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the TODOs sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Answer all the markdown/text cells with \"A: \" on them. The answer must strictly consume one line only.\n",
    "* You are expected to search how to some functions work on the Internet or via the docs. \n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You are expected to understand the data set loading and processing separately from this class.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Classification\n",
    "\n",
    "For this first section, we will create a decision tree to predict the flower species from the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:**\n",
    "The iris data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Sepal length in cm\n",
    "2. Sepal width in cm\n",
    "3. Petal length in cm\n",
    "4. Petal width in cm\n",
    "5. Class (Species):\n",
    "    - Iris Setosa\n",
    "    - Iris Versicolour\n",
    "    - Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loads csv file into a pandas dataframe\n",
    "iris = pd.read_csv('iris.csv')\n",
    "\n",
    "iris[\"species\"] = pd.Categorical(iris[\"species\"]).codes\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Extract the feature columns for X, and the label column for y\n",
    "# Hint : For X, look up pandas.drop()\n",
    "#      : You can convert a DataFrame to a matrix using your_dataframe.as_matrix()\n",
    "### START CODE HERE ###\n",
    "X = \n",
    "y = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "```\n",
    "X: (150, 4) \n",
    "y: (150,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Split the dataset into train and test sets\n",
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our classification tree\n",
    "We will be using sklearn's `DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Build our classification tree, do not draw any conclusions in this cell yet\n",
    "### START CODE HERE ###\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our lecture, we learned that trees can overfit by creating a separate node for every single configuration of feature values possible. Let's see if this is true by running predictions on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Run predictions on the train set, get the accuracy\n",
    "### START CODE HERE ###\n",
    "predictions_train = \n",
    "print(None)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** A decision tree without regularization should be able to achieve 100% accuracy on the training set.\n",
    "\n",
    "If not, you might be seeing a 99.9999% accuracy. But if you look at the predictions, you will see that it got the test set predicted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model does with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Run predictions on the test set, get the accuracy\n",
    "### START CODE HERE ###\n",
    "predictions = \n",
    "print()\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the following tree attributes to understand more of how the tree looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc.tree_.node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc.tree_.children_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc.tree_.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc.tree_.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.export_graphviz(dtc,out_file='tree.dot')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view a graph of the tree, open `tree.dot` and paste its contents in <a src=\"http://webgraphviz.com/\">webgraphviz.com</a>. You'll end up with a similar tree like the one below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/E7UJJZk.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Regression\n",
    "\n",
    "For regression, we will generate a dummy dataset following a sin curve so we can visualize the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "np.random.seed(1)\n",
    "X = np.expand_dims(np.random.uniform(-np.pi,np.pi, n_samples),1)\n",
    "y = np.sin(2*X) + np.random.randn(n_samples,1)*0.3\n",
    "\n",
    "num_items = X.shape[0]\n",
    "randIdx = np.arange(num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split the data set, set random seed to 42 so your results matches the sanity check below\n",
    "### START CODE HERE ###\n",
    "X_train, X_test, y_train, y_test = \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View the data set\n",
    "%matplotlib inline\n",
    "plt.scatter(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our regression tree\n",
    "Here we will use `DecisionTreeRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO : Build our classifier, and predict the y-values of the test data\n",
    "### START CODE HERE ###\n",
    "\n",
    "dtr = \n",
    "\n",
    "predictions = \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO : Calculate for the mean squared error, and the absolute error\n",
    "### START CODE HERE ###\n",
    "mse = \n",
    "print(\"Mean Squared Error :\", mse)\n",
    "\n",
    "are = \n",
    "print(\"Absolute Error :\", are)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "```\n",
    "Mean Squared Error : 0.260963821953\n",
    "Absolute Error : 0.414856812765\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize how the our predictions fare against the actual results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_test, y_test, label='Ground Truth')\n",
    "plt.scatter(X_test, predictions, label='Predicted')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing Decision Trees\n",
    "In the absence of regularization, the decision tree will memorize the training set and achieve 0% error. While this is good in terms of bias, it may not generalize well to never before seen data (variance problem)\n",
    "\n",
    "Modify your model to include three ways of regularization:\n",
    "- **Minimum samples split**: If the remaining samples are less than the specifed value then we stop splitting and make it a leaf node\n",
    "- **Max depth**: Restricts the maximum depth of the trees\n",
    "- **Minimum impurity gain**: If the impurity gain is less than the specified value then we stop splitting and make it a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimum samples split**\n",
    "\n",
    "Apply different minimum samples splits to your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You will use this test set for the visualization below, so use this as your test set for the loop below\n",
    "# You will see that it follows the same sin function as our training data\n",
    "X_test_vis = np.expand_dims(np.linspace(-np.pi,np.pi,300),-1)\n",
    "\n",
    "# Some plotting stuff\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# Try it over these values\n",
    "min_samples_split_vals = [2, 4, 6, 10, 15, 20]\n",
    "\n",
    "for val in min_samples_split_vals:\n",
    "    # TODO : Train a decision tree with the indicated minimum samples split val in this loop\n",
    "    ### START CODE HERE ###\n",
    "    dtr = \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.subplot(3,2,plt_ctr)\n",
    "    \n",
    "    # TODO : Predict the values of the test set we created for visualization\n",
    "    #        Then, make a scatter plot with your predictions as the y-coordinate\n",
    "    ### START CODE HERE ###\n",
    "    predictions = \n",
    "    plt.scatter()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.title(\"Min samples = \"+ str(val))\n",
    "    \n",
    "    plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "Your graph should like the one below:\n",
    "<img src=\"https://imgur.com/Wibqyx4.png\" width=\"400px\">\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As we increase the number of minimum samples, the predictions for close/nearby x values tend to get the same y-value. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum depth**\n",
    "\n",
    "Apply different maximum depths to your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth_vals = [2, 4, 6, 10, 15, 20]\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,12))\n",
    "for val in max_depth_vals:\n",
    "    # TODO : Train a decision tree with the indicated max depth val in this loop\n",
    "    ### START CODE HERE ###\n",
    "    dtr = \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.subplot(3,2,plt_ctr)\n",
    "    \n",
    "    # TODO : Predict the values of the test set we created for visualization\n",
    "    #        Then, make a scatter plot with your predictions as the y-coordinate\n",
    "    ### START CODE HERE ###\n",
    "    predictions = \n",
    "    plt.scatter()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.title(\"Max depth = \"+ str(val))\n",
    "    \n",
    "    plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** The higher the max depth, the more unique labels you will get. (Inverse of minimum spits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As we increase the maximum depth, the predictions for close/nearby x values tend to get the different y-values. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimum impurity**\n",
    "\n",
    "Apply different minimum impurities to your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_impurity_vals = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "plt_ctr = 1\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "for val in min_impurity_vals:\n",
    "    # TODO : Train a decision tree with the indicated minimum impurity val in this loop\n",
    "    ### START CODE HERE ###\n",
    "    dtr = \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.subplot(3,2,plt_ctr)\n",
    "    \n",
    "    # TODO : Predict the values of the test set we created for visualization\n",
    "    #        Then, make a scatter plot with your predictions as the y-coordinate\n",
    "    ### START CODE HERE ###\n",
    "    predictions = \n",
    "    plt.scatter()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    plt.title(\"Min impurity = \"+ str(val))\n",
    "    \n",
    "    plt_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** You should see an output with a similar pattern as minimum samples split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As we increase the minimum impurity, the predictions for close/nearby x values tend to get the same y-value. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **summarize:**\n",
    "\n",
    "\n",
    "* Decision trees are models that partition your data one feature and one feature threshold/value at a time. This creates a tree that acts like a step-by-step \"flowchart\" of what to label a new test instance. It is also for this reason that decision trees are non-linear, the end result is a string of decisions across different features. Because of the visualization we get from decision trees, we can thoroughly understand why an instance labelled that way it was by following the \"prediction path\" the test instance made in the model.\n",
    "\n",
    "* Decision trees are easy to overfit: it can just continue to split/branch off until each instance has its own separate leaf node. We can apply regularization methods like the ones we use above to \n",
    "\n",
    "* Ovrefitting, as we have learned, is a sign of a high variance model. We can prevent the model from exhibiting this by applying bagging, where we create multiple overfit trees and aggregate their results.\n",
    "\n",
    "* Bagging is an example of an ensemble method, like boosting and stacking. You can try them out using sklearn's `ensemble` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>fin</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->\n",
    "\n",
    "<sup>made/compiled by daniel stanley tan & courtney anne ngo üê∞ & thomas james tiam-lee</sup> <br>\n",
    "<sup>for comments, corrections, suggestions, please email:</sup><sup> danieltan07@gmail.com & courtneyngo@gmail.com & thomasjamestiamlee@gmail.com</sup><br>\n",
    "<sup>please cc your instructor, too</sup>\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
